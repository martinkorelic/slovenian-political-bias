{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3mRyXJABvQm"
      },
      "source": [
        "# Modeliranje tem\n",
        "\n",
        "## Okolje\n",
        "\n",
        "Vzpostavitev okolja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h1jsXVwCyHd"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "4zApDenPlYms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Tn4J4JIBulX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import os.path\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification, pipeline\n",
        "from bertopic import BERTopic\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSu7MRUkEUDD"
      },
      "outputs": [],
      "source": [
        "# Setting constants\n",
        "\n",
        "LOCAL = False\n",
        "\n",
        "google_data_dir = \"/content/drive/MyDrive/Diploma/Data\"\n",
        "local_data_dir = \"/data\"\n",
        "\n",
        "root_dir = \"\"\n",
        "if LOCAL:\n",
        "    root_dir = local_data_dir\n",
        "else:\n",
        "    root_dir = google_data_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eHCHjqKD4jR"
      },
      "source": [
        "## Funkcije in razredi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUGMXt-RE2mY"
      },
      "outputs": [],
      "source": [
        "def load_tweets(file_name):\n",
        "\n",
        "  # Load data\n",
        "  data = []\n",
        "\n",
        "  with open(file_name, 'r', encoding='utf8') as sample_data:\n",
        "    data = json.load(sample_data)\n",
        "\n",
        "  return data\n",
        "\n",
        "def save_tweets(data, dir, file_name):\n",
        "  with open(f'{dir}/{file_name}.json', 'w+', encoding='utf8') as outdata:\n",
        "    json.dump(data, outdata, ensure_ascii=False)\n",
        "\n",
        "def load_and_preprocess(cpipeline, data_dir, only_load, tweet_stop_words=[], tweet_upos=[], min_words=4, verbose=False, debug=False):\n",
        "  d = []\n",
        "  if only_load:\n",
        "    d = load_tweets(data_dir)\n",
        "  else:\n",
        "    d = []#preprocess_tweets(cpipeline, load_tweets(data_dir), tweet_stop_words=tweet_stop_words, tweet_upos=tweet_upos, min_words=min_words, verbose=verbose, debug=debug)\n",
        "  return d\n",
        "\n",
        "def load_labelled_tweets(dir, topic_names, shuffle_arrays=True, random_state=77):\n",
        "  topics = []\n",
        "  for t in topic_names:\n",
        "    with open(f'{dir}/labelled_topics/topic_{t}.json', 'r', encoding='utf8') as topic_data:\n",
        "      data = json.load(topic_data)\n",
        "      topics.extend(data)\n",
        "  \n",
        "  topic_lemmas = []\n",
        "  topic_labels = []\n",
        "\n",
        "  for t in topics:\n",
        "    topic_lemmas.append(t['lemma_text'])\n",
        "    topic_labels.append(t['topic'])\n",
        "\n",
        "  topic_labels = [ topic_names.index(x) for x in topic_labels]\n",
        "\n",
        "  if shuffle_arrays:\n",
        "    shuffle(topic_lemmas, topic_labels, random_state=random_state)\n",
        "  return topic_lemmas, topic_labels\n",
        "\n",
        "\"\"\"\n",
        "Bertopic model for modeling topics\n",
        "\n",
        "\"\"\"\n",
        "class BertopicModel:\n",
        "\n",
        "  def __init__(self, model_name, embed_model, config):\n",
        "    self.model_name = model_name\n",
        "    self.embed_model = embed_model\n",
        "    self.config = config\n",
        "\n",
        "    # Create the Bertopic model with config\n",
        "    self.make_model()\n",
        "\n",
        "  def make_model(self):\n",
        "    self.umap_model = UMAP(**self.config[\"umap_conf\"])\n",
        "    self.hdbscan_model = HDBSCAN(**self.config[\"hdbscan_conf\"])\n",
        "    self.bertopic = BERTopic(embedding_model=self.embed_model, umap_model=self.umap_model, hdbscan_model=self.hdbscan_model, **self.config[\"bertopic_conf\"])\n",
        "\n",
        "  def load_tweet_data(self, tweet_data):\n",
        "    doc_tweet_lemmas = [ t['lemma_text'] for t in tweet_data ]\n",
        "    \n",
        "    self.data = {}\n",
        "    self.data[\"tweets\"] = tweet_data\n",
        "    self.data[\"docs\"] = doc_tweet_lemmas\n",
        "\n",
        "  def load_topic_data(self, topic_docs, topic_labels):\n",
        "    if not hasattr(self, 'data'):\n",
        "      self.data = {}\n",
        "      \n",
        "    self.data[\"docs\"] = topic_docs\n",
        "    self.data[\"labels\"] = topic_labels\n",
        "\n",
        "  def train_model(self, only_fit):\n",
        "    data_keys = self.data.keys()\n",
        "\n",
        "    if \"docs\" in data_keys and not only_fit:\n",
        "      topics, probs = self.bertopic.fit_transform(self.data[\"docs\"])\n",
        "      self.result = {}\n",
        "      self.result[\"topic_ids\"] = topics\n",
        "      self.result[\"topic_probs\"] = probs\n",
        "    elif \"docs\" in data_keys and \"labels\" in data_keys and only_fit:\n",
        "      self.bertopic = self.bertopic.fit(self.data[\"docs\"], y=self.data[\"labels\"])\n",
        "    else:\n",
        "      print(\"Error: Missing data!\")\n",
        "  \n",
        "  def predict(self):\n",
        "    data_keys = self.data.keys()\n",
        "\n",
        "    if \"docs\" in data_keys:\n",
        "      topics, probs = self.bertopic.transform(self.data[\"docs\"])\n",
        "      self.result = {}\n",
        "      self.result[\"topic_ids\"] = topics\n",
        "      self.result[\"topic_probs\"] = probs\n",
        "\n",
        "  def reduce(self, nr):\n",
        "    if hasattr(self, 'data') and hasattr(self, 'result'):\n",
        "      topics, probs = self.bertopic.reduce_topics(self.data[\"docs\"], self.data[\"labels\"], nr_topics=nr)\n",
        "      self.result[\"topic_ids\"] = topics\n",
        "      self.result[\"topic_probs\"] = probs\n",
        "\n",
        "  def merge_topics(self, indexes):\n",
        "    if hasattr(self, 'data'):\n",
        "      self.bertopic.merge_topics(self.data[\"docs\"], self.data[\"labels\"], indexes)\n",
        "\n",
        "  def tweets_from_topic(self, ntopic):\n",
        "    if self.result:\n",
        "      tw = []\n",
        "\n",
        "      for i, x in enumerate(self.result.topic_ids):\n",
        "        if x == ntopic:\n",
        "          tw.append(self.data[\"docs\"][i])\n",
        "\n",
        "      return tw\n",
        "    else:\n",
        "      print(\"Error: Missing data!\")\n",
        "      return []\n",
        "\n",
        "  def extract_topics_by_keywords(self, topic_keywords, similar_kw=2):\n",
        "    if hasattr(self, 'result'):\n",
        "\n",
        "      topic_indices = []\n",
        "      topics = self.bertopic.get_topics()\n",
        "      for t in topics:\n",
        "        cur_topics = topics[t]\n",
        "\n",
        "        valid = False\n",
        "        s = 0\n",
        "        for tkws in topic_keywords:\n",
        "          for w, p in cur_topics:\n",
        "            if w in tkws:\n",
        "              s = s + 1\n",
        "              if s >= similar_kw:\n",
        "                valid = True\n",
        "                break\n",
        "          \n",
        "          if valid:\n",
        "            topic_indices.append(t)\n",
        "            break\n",
        "\n",
        "      tweets = []\n",
        "\n",
        "      for i, t in enumerate(self.result[\"topic_ids\"]):\n",
        "        if t in topic_indices:\n",
        "          tweets.append(self.data[\"tweets\"][i])\n",
        "\n",
        "      return tweets\n",
        "\n",
        "    else:\n",
        "      print(\"Error: Missing data!\")\n",
        "      return []\n",
        "\n",
        "\n",
        "  def collect_topic_indices(self, ntopic, tweet_prob=0.5):\n",
        "    if hasattr(self, 'result'):\n",
        "\n",
        "      tweet_ids = []\n",
        "      for i, x in enumerate(self.result[\"topic_ids\"]):\n",
        "\n",
        "        # Check if topic id and probability higher\n",
        "        if ntopic == x and self.result[\"topic_probs\"][i] > tweet_prob:\n",
        "          tweet_ids.append(i)\n",
        "\n",
        "      return tweet_ids\n",
        "    else:\n",
        "      print(\"Error: Missing data!\")\n",
        "      return []\n",
        "\n",
        "  def find_politic_topics(self, keywords, topn=3, sim_threshold=0.5, tweet_prob=0.5, include_prob=False):\n",
        "    if hasattr(self, 'bertopic'):\n",
        "      indices = set()\n",
        "\n",
        "      # Find relating topics\n",
        "      for keyword in keywords:\n",
        "        sim_ids, sim_probs = self.bertopic.find_topics(keyword, top_n=topn)\n",
        "\n",
        "        # Filter based on similarity\n",
        "        sim_topics = [ sim_ids[i] for i, x in enumerate(sim_probs) if x > sim_threshold ]\n",
        "\n",
        "        if len(sim_topics) > 0:\n",
        "          for topic in sim_topics:\n",
        "            indices.update(self.collect_topic_indices(topic, tweet_prob=tweet_prob))\n",
        "      \n",
        "      tweet_docs = []\n",
        "      for i in indices:\n",
        "        tdoc = self.data[\"tweets\"][i]\n",
        "        if include_prob:\n",
        "          tdoc[\"topic_probability\"] = self.result[\"topic_probs\"][i]\n",
        "\n",
        "        tweet_docs.append(tdoc)\n",
        "\n",
        "      return tweet_docs\n",
        "    else:\n",
        "      print(\"Error: Missing data!\")\n",
        "      return []\n",
        "\n",
        "  def visualize(self, t='distance_map'):\n",
        "    if hasattr(self, 'bertopic'):\n",
        "      #return self.bertopic.visualize_topics()\n",
        "      if t == 'barchart':\n",
        "        return self.bertopic.visualize_barchart()\n",
        "      elif t == 'hierarchy':\n",
        "        return self.bertopic.visualize_hierarchy()\n",
        "      elif t == 'heatmap':\n",
        "        return self.bertopic.visualize_heatmap()\n",
        "      elif t == 'term_rank':\n",
        "        return self.bertopic.visualize_term_rank()\n",
        "      else:\n",
        "        return self.bertopic.visualize_topics()\n",
        "      #elif t == 'documents':\n",
        "      #  self.bertopic.visualize_documents()\n",
        "    else:\n",
        "      print(\"Error: Model not yet initiated!\")\n",
        "\n",
        "  def save_model(self, model_dir):\n",
        "    self.bertopic.save(str(model_dir + self.model_name))\n",
        "  \n",
        "  def load_model(self, model_dir):\n",
        "    self.bertopic.load(str(model_dir + self.model_name), embedding_model=self.embed_model)\n",
        "\n",
        "def label_politic_tweets(model : BertopicModel, topic_info, data_dir, topn=3, n_sim_subtopics=3, save_tweets=False, verbose=True):\n",
        "\n",
        "  if verbose:\n",
        "    print(f'-- Collected batch topic distribution summary:')\n",
        "\n",
        "  sim_topics = similar_topics(model, topic_info, topn=topn, n_sim_subtopics=n_sim_subtopics)\n",
        "  #topics = [ x for x in topic_info]\n",
        "\n",
        "  tweets = model.data['tweets']\n",
        "  labels = model.result['topic_ids']\n",
        "  probs = model.result['topic_probs']\n",
        "\n",
        "  for i in range(len(tweets)):\n",
        "    tweet = tweets[i]\n",
        "    label = labels[i]\n",
        "    prob = probs[i]\n",
        "\n",
        "    most_likely_topic = None\n",
        "    most_likely_prob = 0\n",
        "\n",
        "    for st in sim_topics:\n",
        "      for sbt, prob in sim_topics[st]:\n",
        "        if sbt == label and prob > most_likely_prob:\n",
        "          most_likely_topic = st\n",
        "          most_likely_prob = prob\n",
        "\n",
        "\n",
        "    if most_likely_topic is not None and topic_info[most_likely_topic]['strict']:\n",
        "      kw = []\n",
        "      kw.extend(topic_info[most_likely_topic]['keywords'])\n",
        "      kw.extend(topic_info[most_likely_topic]['search_term'])\n",
        "\n",
        "      founds = False\n",
        "      for key in kw:\n",
        "        if key in tweet['lemma_text']:\n",
        "          tweet['topic'] = most_likely_topic\n",
        "          tweet['topic_probability'] = prob\n",
        "          founds = True\n",
        "          break\n",
        "      \n",
        "      if not founds:\n",
        "        tweet['topic'] = None\n",
        "        tweet['topic_probability'] = 0\n",
        "\n",
        "    else:\n",
        "      tweet['topic'] = most_likely_topic\n",
        "      tweet['topic_probability'] = prob\n",
        "\n",
        "  if save_tweets:\n",
        "    for tp in topic_info:\n",
        "      t = list(filter(lambda x: x['topic'] == tp, tweets))\n",
        "      if verbose:\n",
        "        print(f'-- {tp} : {len(t)}')\n",
        "      overwrite_labelled_topics(tp, t, data_dir)\n",
        "\n",
        "  return model.data['tweets']\n",
        "\n",
        "def similar_topics(model : BertopicModel, topic_info, topn=3, n_sim_subtopics=3) -> dict:\n",
        "    \n",
        "  sim_topics = {}\n",
        "\n",
        "  for topic in topic_info:\n",
        "    tt = topic_info[topic]\n",
        "\n",
        "    tpcs1 = {}\n",
        "    for keyword in tt['search_term']:\n",
        "      sims = model.bertopic.find_topics(keyword, top_n=topn)\n",
        "\n",
        "      sims = tuple(zip(sims[0], sims[1]))\n",
        "\n",
        "      tpcs2 = dict((x, y) for x, y in sims)\n",
        "\n",
        "      tpcs1 = {\n",
        "        key: tpcs1.get(key, 0) + tpcs2.get(key, 0) for key in set(tpcs1) | set(tpcs2)\n",
        "      }\n",
        "    \n",
        "    # Normalize\n",
        "    mv = max(tpcs1.values())\n",
        "    for kj in tpcs1:\n",
        "      tpcs1[kj] = float(tpcs1[kj] / mv)\n",
        "      \n",
        "    subtopics = []\n",
        "    for i in range(n_sim_subtopics):\n",
        "      if tpcs1:\n",
        "        k1 = max(tpcs1, key=tpcs1.get)\n",
        "        if k1 != -1:\n",
        "          subtopics.append((k1, tpcs1[k1]))\n",
        "        tpcs1.pop(k1)\n",
        "    \n",
        "    sim_topics[topic] = subtopics\n",
        "  \n",
        "  return sim_topics\n",
        "\n",
        "def overwrite_labelled_topics(file_topic, topic_tweets, data_dir):\n",
        "    data=[]\n",
        "    with open(f'{data_dir}/labelled_topics/topic_{file_topic}.json', 'r', encoding='utf8') as topic_data:\n",
        "      data = json.load(topic_data)\n",
        "      data.extend(topic_tweets)\n",
        "    with open(f'{data_dir}/labelled_topics/topic_{file_topic}.json', 'w', encoding='utf8') as topic_data_n:\n",
        "      json.dump(data, topic_data_n, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wDbqNgPjAsX"
      },
      "outputs": [],
      "source": [
        "# Setting configuration\n",
        "\n",
        "# Path to preprocessed tweet data\n",
        "\n",
        "YEAR = 2021\n",
        "EPOCH = 41\n",
        "BATCHES = (1,11)\n",
        "\n",
        "#tweet_data_path = \n",
        "topics = ['splav', 'lgbtq', 'begunci', 'religija', 'krscanstvo', 'militarizem', 'varnost', 'denacionalizacija', 'levo', 'desno']\n",
        "\n",
        "# Path to save labelled tweet data\n",
        "SAVING = True\n",
        "tweet_save_path = f'stpt/{YEAR}-{EPOCH}'\n",
        "\n",
        "# Verbose\n",
        "VERBOSE = True\n",
        "\n",
        "# Imported configurations\n",
        "topic_config_path = f'{root_dir}/configs/topics.json'\n",
        "politics_seed_topic = load_tweets(topic_config_path)\n",
        "\n",
        "topic_info = {\n",
        "    'begunci': {\n",
        "        'keywords': [\"migrant\", \"migriranje\", \"beg\", \"begunec\", \"meja\", \"begunci\", \"migracija\"],\n",
        "        'regexes': [r'\\bmigr\\w+', r'\\bbeg\\w+']\n",
        "    },\n",
        "    'lgbt': {\n",
        "        'keywords': [\"lgbtq\", \"lgbt\", \"lgbtqia\", \"istospolen\", \"spol\", \"gej\", \"lezbijka\", \"lezbijski\", \"trans\", \"seksualnost\"],\n",
        "        'regexes': [r'\\btrans\\w+', r'\\bseks\\w+', r'\\blgbt\\w+', r'\\bistospol\\w+', r'\\bgej\\w+', r'\\blezb\\w+', r'\\bspol\\w+']\n",
        "    },\n",
        "    'religija': {\n",
        "        'keywords': [\"musliman\", \"islam\", \"radikalen\", \"islamski\", \"muslimanski\", \"jud\", \"izrael\", \"izraelski\", \"vera\"],\n",
        "        'regexes': [r'\\bver\\w+', r'\\bislam\\w+', r'\\bžid\\w+', r'\\bzid\\w+', r'\\bislam\\w+', r'\\bjud\\w+', r'\\bmusli\\w+']\n",
        "    },\n",
        "    'splav': {\n",
        "        'keywords': [\"splav\", \"zarodek\", \"kontracepcija\", \"vazektomija\", \"sterilizacija\", \"diafragma\", \"kondom\", \"maternica\", \"fetus\"],\n",
        "        'regexes': [r'\\bkontracep\\w+', r'\\bsplav\\w+', r'\\bnoseč\\w+', r'\\bnosec\\w+', r'\\bsteril\\w+', r'\\bkastri\\w+']\n",
        "    },\n",
        "    'levo': {\n",
        "        'keywords': [\"levica\", \"levicar\", \"lev\", \"levičar\", \"mesec\", \"levi\"],\n",
        "        'regexes': [r'\\blev\\w+']\n",
        "    },\n",
        "    'krscanstvo': {\n",
        "        'keywords': [\"cerkev\", \"župnik\", \"verouk\", \"vera\", \"bog\", \"mučenik\", \"vernik\", \"verniki\", \"otrok\", \"papež\"],\n",
        "        'regexes': [r'\\bkrscan\\w+', r'\\bkrščan\\w+', r'\\bcerkv\\w+', r'\\bkatoli\\w+', r'\\bdruzi\\w+', r'\\bdruži\\w+', r'\\bteolo\\w+']\n",
        "    },\n",
        "    'militarizem': {\n",
        "      'keywords': [\"nato\", \"vojska\", \"vojak\", \"meja\", \"obramba\", \"zasčita\", \"sila\", \"varnost\", \"orožje\", \"orozje\"],\n",
        "      'regexes': [r'\\bmilitar\\w+', r'\\bvoj\\w+', r'\\bnaborni\\w+', r'\\bpovelj\\w+', r'\\bzavez\\w+', r'\\bpatri\\w+']\n",
        "    },\n",
        "    'varnost': {\n",
        "      'keywords': [\"represiven\", \"policija\", \"protest\", \"varda\", \"nadzor\", \"varovanje\", \"varnost\", \"varen\", \"red\", \"mir\"],\n",
        "      'regexes': [r'\\bpolici\\w+', r'\\bprotest\\w+', r'\\bshod\\w+', r'\\bteror\\w+']\n",
        "    },\n",
        "    'denacionalizacija': {\n",
        "      'keywords': [\"denacionalizacija\", \"privat\", \"last\", \"premoženje\", \"kapital\"],\n",
        "      'regexes': [r'\\bprivat\\w+', r'\\bzaseb\\w+', r'\\blast\\w+', r'\\bpremož\\w+', r'\\bpodrža\\w+', r'\\bdenacional\\w+', r'\\bkapital\\w+']\n",
        "    },\n",
        "    'desno': {\n",
        "        'keywords': [\"desnica\", \"desno\", \"jj\", \"sds\", \"desničar\", \"janša\", \"jansa\", \"nsi\", \"janez\"],\n",
        "        'regexes': [r'\\bdesni\\w+', r'\\bjan\\w+']\n",
        "    }\n",
        "}\n",
        "\n",
        "# Preprocessing configuration\n",
        "preprocess_config = {\n",
        "    'min_words': 4,\n",
        "    'verbose': True,\n",
        "    'debug': False,\n",
        "    'tweet_upos': ['PUNCT', 'NUM', 'SYM', 'CCONJ', 'INTJ'],\n",
        "    'tweet_stop_words': ['http', 'https', 'rt', 'oz']\n",
        "}\n",
        "\n",
        "# Bertopic first layer configuration\n",
        "bertopic_FL_config = {\n",
        "    'bertopic_conf': {\n",
        "        \"top_n_words\": 5,\n",
        "        \"min_topic_size\": 15,\n",
        "        \"seed_topic_list\": politics_seed_topic\n",
        "    },\n",
        "    'umap_conf': {\n",
        "        \"n_neighbors\": 15,\n",
        "        \"n_components\": 15,\n",
        "        \"metric\": 'cosine'\n",
        "    },\n",
        "    'hdbscan_conf': {\n",
        "        \"min_cluster_size\": 15,\n",
        "        \"metric\": 'euclidean',\n",
        "        \"prediction_data\": True\n",
        "    },\n",
        "    'similar_kw': 1\n",
        "}\n",
        "\n",
        "# Bertopic second layer configuration\n",
        "bertopic_SL_config = {\n",
        "    'bertopic_conf': {\n",
        "        \"top_n_words\": 10,\n",
        "        \"min_topic_size\": 20,\n",
        "        \"n_gram_range\": (1,2),\n",
        "        \"nr_topics\": 10,\n",
        "        \"diversity\": 0.1\n",
        "    },\n",
        "    'umap_conf': {\n",
        "        \"n_neighbors\": 20,\n",
        "        \"n_components\": 15,\n",
        "        \"metric\": 'cosine'\n",
        "    },\n",
        "    'hdbscan_conf': {\n",
        "        \"min_cluster_size\": 15,\n",
        "        \"metric\": 'euclidean',\n",
        "        \"prediction_data\": True\n",
        "    },\n",
        "    'topn': 3,\n",
        "    'n_sim_subtopics': 3,\n",
        "}\n",
        "\n",
        "# Bertopic third layer configuration\n",
        "bertopic_TL_config = {\n",
        "    'bertopic_conf': {\n",
        "        \"top_n_words\": 5,\n",
        "        \"min_topic_size\": 10,\n",
        "        \"n_gram_range\": (1,2),\n",
        "        \"diversity\": 0.1\n",
        "    },\n",
        "    'umap_conf': {\n",
        "        \"n_neighbors\": 10,\n",
        "        \"n_components\": 10,\n",
        "        \"metric\": 'cosine'\n",
        "    },\n",
        "    'hdbscan_conf': {\n",
        "        \"min_cluster_size\": 5,\n",
        "        \"metric\": 'euclidean',\n",
        "        \"prediction_data\": True\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K11pc8m4jAsY"
      },
      "outputs": [],
      "source": [
        "# Loading models\n",
        "\n",
        "#slobert_model = AutoModelForMaskedLM.from_pretrained(\"EMBEDDIA/sloberta\")\n",
        "slobert_model = pipeline('feature-extraction', model='EMBEDDIA/sloberta')#, device=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prvi sloj modeliranja tem"
      ],
      "metadata": {
        "id": "o4_Q0h9_JhYb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWp10rl9jAsZ"
      },
      "outputs": [],
      "source": [
        "# MAIN CODE\n",
        "\n",
        "# Extracting only general politic tweets\n",
        "\n",
        "# Load preprocessed tweets\n",
        "preprocessed_tweet_data = []\n",
        "for b in range(*BATCHES):\n",
        "  preprocessed_tweet_data.extend(load_and_preprocess(None, f'{root_dir}/preprocess/{YEAR}-{EPOCH}/{YEAR}_{EPOCH}_{b}.json', True))\n",
        "\n",
        "# Print summary if verbose\n",
        "if VERBOSE:\n",
        "  print(f'- Batch \"{YEAR}_{EPOCH}\" summary:')\n",
        "  print(f'-- Batch length: {len(preprocessed_tweet_data)}')\n",
        "  \n",
        "# First layer of topic modeling\n",
        "print(f'- First layer of topic modeling...')\n",
        "\n",
        "# Create Bertopic model (1st layer)\n",
        "bt_fl_model = BertopicModel('Bertopic_FL', embed_model=slobert_model, config=bertopic_FL_config)\n",
        "\n",
        "# Load twitter data\n",
        "bt_fl_model.load_tweet_data(preprocessed_tweet_data)\n",
        "\n",
        "if VERBOSE:\n",
        "  print(f'- Training 1st layer of Bertopic model...')\n",
        "\n",
        "# Train the model\n",
        "bt_fl_model.train_model(only_fit=False)\n",
        "\n",
        "if SAVING:\n",
        "  if VERBOSE:\n",
        "    print(f'- Saving 1st layer of Bertopic model...')\n",
        "  # Save model\n",
        "  bt_fl_model.save_model(f'{root_dir}/models')\n",
        "\n",
        "# Visualize\n",
        "bt_fl_model.visualize()\n",
        "\n",
        "# Extract general politic by topic keywords\n",
        "extracted_tweets = bt_fl_model.extract_topics_by_keywords(bertopic_FL_config['bertopic_conf']['seed_topic_list'], similar_kw=bertopic_FL_config['similar_kw'])\n",
        "\n",
        "if VERBOSE:\n",
        "  print(f'- Batch of general politics summary:')\n",
        "  print(f'-- Batch length: {len(extracted_tweets)}')\n",
        "  print(f'-- Extracted general politics tweets: {len(extracted_tweets)}/{len(preprocessed_tweet_data)} ({int((len(extracted_tweets)/len(preprocessed_tweet_data))*100)} %)')\n",
        "\n",
        "# Save STP tweets\n",
        "if SAVING:\n",
        "  if VERBOSE:\n",
        "    print(f'- Saving STPT in file {tweet_save_path}/{YEAR}_{EPOCH}_{b}...')\n",
        "  save_tweets(extracted_tweets, dir=f'{root_dir}/{tweet_save_path}', file_name=f'{YEAR}_{EPOCH}_ALL')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drugi sloj modeliranja tem"
      ],
      "metadata": {
        "id": "kagnlGXRJce9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN CODE\n",
        "\n",
        "# Second layer of topic modeling\n",
        "if VERBOSE:\n",
        "  print(f'- Second layer of topic modeling...')\n",
        "\n",
        "# Get training data\n",
        "X, y = load_labelled_tweets(root_dir, topics)\n",
        "\n",
        "# Modifications\n",
        "\n",
        "#vectorizer_model = CountVectorizer(stop_words = stopwords.words('slovene'), ngram_range=(1, 2), max_features=10)\n",
        "#bertopic_SL_config['bertopic_conf']['vectorizer_model'] = vectorizer_model\n",
        "\n",
        "ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)\n",
        "bertopic_SL_config['bertopic_conf']['ctfidf_model'] = ctfidf_model\n",
        "\n",
        "# Create Bertopic model (2nd layer)\n",
        "bt_sl_model = BertopicModel('Bertopic_SL', embed_model=slobert_model, config=bertopic_SL_config)\n",
        "\n",
        "# Load training data\n",
        "bt_sl_model.load_topic_data(X, y)\n",
        "\n",
        "# Train model with training data\n",
        "if VERBOSE:\n",
        "  print(f'- Training 2nd layer of Bertopic model...')\n",
        "bt_sl_model.train_model(only_fit=True)\n",
        "\n",
        "# Saving the SL model\n",
        "if SAVING:\n",
        "  if VERBOSE:\n",
        "    print(f'- Saving 2nd layer of Bertopic model...')\n",
        "  bt_sl_model.save_model(f'{root_dir}/models')\n",
        "\n",
        "bt_sl_model.visualize()"
      ],
      "metadata": {
        "id": "8My6UJpzSdJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bt_sl_model.bertopic.get_topics()"
      ],
      "metadata": {
        "id": "OWLyW9MYJxQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict new instances\n",
        "\n",
        "topic_names = ['desno', 'levo', 'begunci', 'splav', 'krscanstvo', 'religija', 'militarizem', 'lgbt', 'denacionalizacija', 'varnost']\n",
        "\n",
        "# Load extracted tweets (test data)\n",
        "tweets_to_predict = load_and_preprocess(None, f'{root_dir}/stpt/{YEAR}-{EPOCH}/{YEAR}_{EPOCH}_ALL.json', True)\n",
        "\n",
        "bt_sl_model.load_tweet_data(tweets_to_predict)\n",
        "\n",
        "if VERBOSE:\n",
        "  print(f'- Predicting new instances on second layer topic modeling...')\n",
        "\n",
        "# Predict new instances on test data\n",
        "bt_sl_model.predict()\n",
        "\n",
        "# Label new instances & overwrite\n",
        "if SAVING:\n",
        "  if VERBOSE:\n",
        "    print(f'- Labelling and saving topic tweets...')\n",
        "    #labelled_tweets = label_politic_tweets(bt_sl_model, topic_info, root_dir, topn=bertopic_SL_config['topn'], n_sim_subtopics=bertopic_SL_config['n_sim_subtopics'], save_tweets=SAVING, verbose=VERBOSE)\n",
        "\n",
        "    threshold = 0.4\n",
        "\n",
        "    predicted_topics = zip([ i for i in range(0, len(bt_sl_model.result['topic_ids']))], bt_sl_model.result['topic_ids'], bt_sl_model.result['topic_probs'])\n",
        "\n",
        "    predicted_topics = list(filter(lambda t: t[2] > threshold, predicted_topics))\n",
        "    for i in range(len(topic_names)):\n",
        "      topic_tweet_ids = list(filter(lambda x: x[1] == i, predicted_topics))\n",
        "\n",
        "    topic_tweets = []\n",
        "    for idx, ti, p in topic_tweet_ids:\n",
        "      tw = bt_sl_model.data['tweets'][idx]\n",
        "      tw['topic'] = topic_names[ti]\n",
        "      tw['topic_probability'] = p\n",
        "      topic_tweets.append(tw)\n",
        "\n",
        "    data=[]\n",
        "    with open(f'{root_dir}/process/tweets_{topic_names[i]}.json', 'r', encoding='utf8') as topic_data:\n",
        "      data = json.load(topic_data)\n",
        "      data.extend(topic_tweets)\n",
        "    with open(f'{root_dir}/process/tweets_{topic_names[i]}.json', 'w', encoding='utf8') as topic_data_n:\n",
        "      json.dump(data, topic_data_n, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "QA1x5g3MA6P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postprocessing"
      ],
      "metadata": {
        "id": "UDqsISHNJZPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_tweets = []\n",
        "\n",
        "topic_to_postprocess = 'levo'\n",
        "#topic_seed_list = [politics_seed_topic[9]]\n",
        "#bertopic_TL_config['bertopic_conf']['seed_topic_list'] = topic_seed_list\n",
        "\n",
        "processed_tweets.extend(load_and_preprocess(None, f'{root_dir}/process/tweets_{topic_to_postprocess}.json', True))\n",
        "\n",
        "# Print summary if verbose\n",
        "if VERBOSE:\n",
        "  print(f'- Tweets \"{topic_to_postprocess}\" summary:')\n",
        "  print(f'-- Batch length: {len(processed_tweets)}')\n",
        "  \n",
        "# Postprocessing\n",
        "print(f'- Postprocess start...')\n",
        "\n",
        "# Modifications\n",
        "ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)\n",
        "bertopic_TL_config['bertopic_conf']['ctfidf_model'] = ctfidf_model\n",
        "\n",
        "# Create Bertopic model (3rd layer)\n",
        "bt_tl_model = BertopicModel('Bertopic_TL', embed_model=slobert_model, config=bertopic_TL_config)\n",
        "\n",
        "# Load twitter data\n",
        "bt_tl_model.load_tweet_data(processed_tweets)\n",
        "\n",
        "# Train\n",
        "if VERBOSE:\n",
        "  print(f'- Training 3rd layer of Bertopic model...')\n",
        "bt_tl_model.train_model(only_fit=False)\n",
        "\n",
        "# Visualize\n",
        "bt_tl_model.visualize()"
      ],
      "metadata": {
        "id": "_WI5f9azJY8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bt_tl_model.bertopic.get_topic_info()"
      ],
      "metadata": {
        "id": "qHBuuF9EN_Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually eliminate irrelevant topics\n",
        "\n",
        "topics_to_eliminate = [-1]\n",
        "\n",
        "predicted_topics = zip([ i for i in range(0, len(bt_tl_model.result['topic_ids']))], bt_tl_model.result['topic_ids'], bt_tl_model.result['topic_probs'])\n",
        "\n",
        "predicted_topics = list(filter(lambda t: t[1] in topics_to_eliminate, predicted_topics))\n",
        "\n",
        "topic_tweets = []\n",
        "for idx, ti, p in predicted_topics:\n",
        "  print(bt_tl_model.data['tweets'][idx]['raw_text'])\n",
        "  topic_tweets.append(bt_tl_model.data['tweets'][idx])"
      ],
      "metadata": {
        "id": "cYmDUhnEO1Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overwrite and save postprocessed data\n",
        "\n",
        "print(f'Batch length after postprocessing: {len(topic_tweets)}/{len(processed_tweets)}')\n",
        "\n",
        "data=[]\n",
        "with open(f'{root_dir}/postprocess/tweets_{topic_to_postprocess}.json', 'r', encoding='utf8') as topic_data:\n",
        "  data = json.load(topic_data)\n",
        "  data.extend(topic_tweets)\n",
        "with open(f'{root_dir}/postprocess/tweets_{topic_to_postprocess}.json', 'w', encoding='utf8') as topic_data_n:\n",
        "  json.dump(data, topic_data_n, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "xqJ1Vw9zgdGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another level of postprocessing if needed\n",
        "\n",
        "# Print summary if verbose\n",
        "if VERBOSE:\n",
        "  print(f'- Tweets \"{topic_to_postprocess}\" (extra) summary:')\n",
        "  print(f'-- Batch length: {len(topic_tweets)}')\n",
        "  \n",
        "# Postprocessing\n",
        "print(f'- Postprocess start...')\n",
        "\n",
        "# Create Bertopic model (3rd layer extra)\n",
        "bt_tlx_model = BertopicModel('Bertopic_TLx', embed_model=slobert_model, config=bertopic_TL_config)\n",
        "\n",
        "# Load twitter data\n",
        "bt_tlx_model.load_tweet_data(topic_tweets)\n",
        "\n",
        "# Modifications\n",
        "ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)\n",
        "bertopic_TL_config['bertopic_conf']['ctfidf_model'] = ctfidf_model\n",
        "\n",
        "# Train\n",
        "if VERBOSE:\n",
        "  print(f'- Training 3rd (extra) layer of Bertopic model...')\n",
        "bt_tlx_model.train_model(only_fit=False)\n",
        "\n",
        "# Visualize\n",
        "bt_tlx_model.visualize()"
      ],
      "metadata": {
        "id": "Ead0cqbUc4ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bt_tlx_model.bertopic.get_topic_info()"
      ],
      "metadata": {
        "id": "rrNDf268etm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually eliminate irrelevant topics\n",
        "\n",
        "topics_to_eliminate = [0,1,2,3,4,6,7,8,9,10,11,15,16]\n",
        "\n",
        "predicted_topics_x = zip([ i for i in range(0, len(bt_tlx_model.result['topic_ids']))], bt_tlx_model.result['topic_ids'], bt_tlx_model.result['topic_probs'])\n",
        "\n",
        "predicted_topics_x = list(filter(lambda t: t[1] in topics_to_eliminate, predicted_topics_x))\n",
        "\n",
        "topic_tweets = []\n",
        "for idx, ti, p in predicted_topics_x:\n",
        "  print(bt_tlx_model.data['tweets'][idx]['raw_text'])\n",
        "  topic_tweets.append(bt_tlx_model.data['tweets'][idx])"
      ],
      "metadata": {
        "id": "P88D4SmjdF_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overwrite and save postprocessed data\n",
        "\n",
        "print(f'Batch length after postprocessing: {len(topic_tweets)}/{len(processed_tweets)}')\n",
        "\n",
        "data=[]\n",
        "with open(f'{root_dir}/postprocess/tweets_{topic_to_postprocess}.json', 'r', encoding='utf8') as topic_data:\n",
        "  data = json.load(topic_data)\n",
        "  data.extend(topic_tweets)\n",
        "with open(f'{root_dir}/postprocess/tweets_{topic_to_postprocess}.json', 'w', encoding='utf8') as topic_data_n:\n",
        "  json.dump(data, topic_data_n, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "rVABmeLrgevO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final step - find by keywords and regexes (optional)\n",
        "\n",
        "topics_to_eliminate = [-1]\n",
        "\n",
        "predicted_topics_x = zip([ i for i in range(0, len(bt_tlx_model.result['topic_ids']))], bt_tlx_model.result['topic_ids'], bt_tlx_model.result['topic_probs'])\n",
        "\n",
        "predicted_topics_x = list(filter(lambda t: t[1] in topics_to_eliminate, predicted_topics_x))\n",
        "\n",
        "topic_tweets_d = []\n",
        "for idx, ti, p in predicted_topics_x:\n",
        "  topic_tweets_d.append(bt_tlx_model.data['tweets'][idx])\n",
        "\n",
        "\n",
        "topic_tweets = []\n",
        "for tw in topic_tweets_d:\n",
        "\n",
        "  isvalid = False\n",
        "\n",
        "  lemmas = tw['lemma_text'].split(\" \")\n",
        "\n",
        "  for k in topic_info[topic_to_postprocess]['keywords']:\n",
        "    if k in lemmas:\n",
        "      isvalid = True\n",
        "      topic_tweets.append(tw)\n",
        "      break\n",
        "    \n",
        "  if not isvalid:\n",
        "    for r in topic_info[topic_to_postprocess]['regexes']:\n",
        "      found = re.search(r, tw['lemma_text'])\n",
        "\n",
        "      if found:\n",
        "        isvalid = True\n",
        "        topic_tweets.append(tw)\n",
        "        break\n",
        "  \n",
        "  if isvalid:\n",
        "    print(tw['raw_text'])"
      ],
      "metadata": {
        "id": "zxZC8XgEf76_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overwrite and save postprocessed data\n",
        "print(f'Batch length after postprocessing: {len(topic_tweets)}/{len(processed_tweets)}')\n",
        "\n",
        "data=[]\n",
        "with open(f'{root_dir}/postprocess/tweets_{topic_to_postprocess}.json', 'r', encoding='utf8') as topic_data:\n",
        "  data = json.load(topic_data)\n",
        "  data.extend(topic_tweets)\n",
        "with open(f'{root_dir}/postprocess/tweets_{topic_to_postprocess}.json', 'w', encoding='utf8') as topic_data_n:\n",
        "  json.dump(data, topic_data_n, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "2wTKHKZxXwkc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "a37e6ef41f9041754ceb21f3dda61e636f9e402d0d6f0468955885061f3c932e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}