{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3mRyXJABvQm"
      },
      "source": [
        "# Predobdelava podatkov\n",
        "\n",
        "## Okolje\n",
        "\n",
        "Vzpostavitev okolja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h1jsXVwCyHd",
        "outputId": "82a62a9c-6e35-4cf5-b861-f9ea19422d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting classla\n",
            "  Downloading classla-1.2.0-py3-none-any.whl (275 kB)\n",
            "\u001b[K     |████████████████████████████████| 275 kB 32.5 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.62.3\n",
            "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting classla\n",
            "  Downloading classla-1.1.1-py3-none-any.whl (262 kB)\n",
            "\u001b[K     |████████████████████████████████| 262 kB 74.1 MB/s \n",
            "\u001b[?25hCollecting requests==2.27.1\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting protobuf==3.19.3\n",
            "  Downloading protobuf-3.19.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 45.8 MB/s \n",
            "\u001b[?25hCollecting reldi-tokeniser==1.0.1\n",
            "  Downloading reldi_tokeniser-1.0.1-py3-none-any.whl (16 kB)\n",
            "Collecting torch==1.10.1\n",
            "  Downloading torch-1.10.1-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:39tcmalloc: large alloc 1147494400 bytes == 0x39bb8000 @  0x7f1c634d3615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 15 kB/s \n",
            "\u001b[?25hCollecting classla\n",
            "  Downloading classla-1.1.0-py3-none-any.whl (262 kB)\n",
            "\u001b[K     |████████████████████████████████| 262 kB 75.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from classla) (1.21.6)\n",
            "Collecting reldi-tokeniser\n",
            "  Downloading reldi_tokeniser-1.0.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from classla) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from classla) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from classla) (3.17.3)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from classla) (1.12.0+cu113)\n",
            "Collecting obeliks>=1.1.3\n",
            "  Downloading obeliks-1.1.6-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from obeliks>=1.1.3->classla) (4.9.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from obeliks>=1.1.3->classla) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->classla) (4.1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->classla) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->classla) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->classla) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->classla) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->classla) (3.0.4)\n",
            "Installing collected packages: reldi-tokeniser, obeliks, classla\n",
            "Successfully installed classla-1.1.0 obeliks-1.1.6 reldi-tokeniser-1.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tweet-preprocessor\n",
        "!pip install classla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Tn4J4JIBulX",
        "outputId": "2dca5b47-dea2-4a1c-d533-f47082570c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Downloading https://raw.githubusercontent.com/clarinsi/classla-resources/main/resources_1.0.1.json: 10.3kB [00:00, 5.58MB/s]                   \n",
            "2022-08-09 12:03:55 INFO: Downloading these customized packages for language: sl (Slovenian)...\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | standard |\n",
            "| pos       | standard |\n",
            "| lemma     | standard |\n",
            "| depparse  | standard |\n",
            "| ner       | standard |\n",
            "| pretrain  | standard |\n",
            "========================\n",
            "\n",
            "Downloading https://www.clarin.si/repository/xmlui/bitstream/handle/11356/1476/ssj500k: 100%|██████████| 218M/218M [01:12<00:00, 3.00MB/s]\n",
            "Downloading https://www.clarin.si/repository/xmlui/bitstream/handle/11356/1478/sl_all_Sloleks_lemmatizer.pt: 100%|██████████| 2.02M/2.02M [00:01<00:00, 1.23MB/s]\n",
            "Downloading https://www.clarin.si/repository/xmlui/bitstream/handle/11356/1258/ssj500k_ud: 100%|██████████| 96.0M/96.0M [00:30<00:00, 3.13MB/s]\n",
            "Downloading https://www.clarin.si/repository/xmlui/bitstream/handle/11356/1321/ssj500k: 100%|██████████| 48.3M/48.3M [00:16<00:00, 3.02MB/s]\n",
            "Downloading https://www.clarin.si/repository/xmlui/bitstream/handle/11356/1476/ssj500k.pretrain.pt: 100%|██████████| 110M/110M [00:38<00:00, 2.86MB/s]\n",
            "2022-08-09 12:06:48 INFO: Finished downloading models and saved to /root/classla_resources.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import json\n",
        "import classla\n",
        "import re\n",
        "import time\n",
        "\n",
        "classla.download('sl')\n",
        "\n",
        "import preprocessor as tpre\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JSu7MRUkEUDD"
      },
      "outputs": [],
      "source": [
        "# Setting constants\n",
        "\n",
        "LOCAL = False\n",
        "\n",
        "google_data_dir = \"/content/drive/MyDrive/Diploma/Data\"\n",
        "local_data_dir = \"/data\"\n",
        "\n",
        "root_dir = \"\"\n",
        "if LOCAL:\n",
        "    root_dir = local_data_dir\n",
        "else:\n",
        "    root_dir = google_data_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eHCHjqKD4jR"
      },
      "source": [
        "## Funkcije in razredi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nUGMXt-RE2mY"
      },
      "outputs": [],
      "source": [
        "def preprocess_tweets(preprocess_pipeline, tweets, tweet_stop_words=[], tweet_upos=[], min_words=4, verbose=False, debug=False):\n",
        "\n",
        "  stop_words = stopwords.words('slovene')\n",
        "  stop_words.extend(tweet_stop_words)\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop_words = list(set(stop_words))\n",
        "\n",
        "  data = []\n",
        "  start_time = 0\n",
        "\n",
        "  for index, tweet in enumerate(tweets):\n",
        "\n",
        "    if index%1000 == 0 and verbose:\n",
        "      # Import time\n",
        "      print(f'-- Progress: {index}/{len(tweets)}')\n",
        "      if index >= 1000:\n",
        "        print(f'-- Time elapsed: {time.time() - start_time}s')\n",
        "        print(f'-- Tweets preprocessed: {len(data)}')\n",
        "      start_time = time.time()\n",
        "\n",
        "    # Take attributes\n",
        "    tweet_full_text = tweet['full_text']\n",
        "\n",
        "    # Skip if retweet\n",
        "    if tweet_full_text.startswith(\"RT\"):\n",
        "      continue\n",
        "\n",
        "    tweet_id = tweet['id']\n",
        "    #tweet_hashtags = tweet['entities']['hashtags']\n",
        "    #tweet_mentions = tweet['entities']['mentions']\n",
        "    tweet_hashtags = tweet['hashtags']\n",
        "    tweet_mentions = tweet['mentions']\n",
        "    tweet_created_at = tweet['created_at']\n",
        "\n",
        "    tweet_user_name = tweet['user']['name']\n",
        "    tweet_user_screen_name = tweet['user']['screen_name']\n",
        "    tweet_user_description = clean_tweet_text(tweet['user']['description']).lower()\n",
        "\n",
        "    # Remove hashtags, mentions, links, emojis and others\n",
        "    tweet_full_text = clean_tweet_text(tweet_full_text)\n",
        "\n",
        "    if debug:\n",
        "      print(f'Raw text:\\n{tweet_full_text}')\n",
        "\n",
        "    # Preprocess with preprocessing pipeline\n",
        "    tweet_lemma_text = preprocess_pipeline(tweet_full_text)\n",
        "\n",
        "    tweet_processed_lemmas = []\n",
        "    for sentence in tweet_lemma_text.sentences:\n",
        "\n",
        "      sentence_words = []\n",
        "\n",
        "      for i, word in enumerate(sentence.words):\n",
        " \n",
        "        # If the tweet is a retweet\n",
        "        if word.lemma.lower() == 'rt' and i == 0:\n",
        "          break\n",
        "\n",
        "        if debug:\n",
        "          print(f'Lemma: {word.lemma.lower()} -------------- Upos: {word.upos}')\n",
        "\n",
        "        # Not punctuation and not number + clean stopwords\n",
        "        if word.upos not in tweet_upos and word.lemma.lower() not in stop_words:\n",
        "          sentence_words.append(word.lemma.lower())\n",
        "        elif debug:\n",
        "          print(f'Discarded word: {word.lemma}')\n",
        "\n",
        "      tweet_processed_lemmas.extend(sentence_words)\n",
        "    \n",
        "    if debug:\n",
        "      print(f'Preprocessed lemmas:\\n{tweet_processed_lemmas}')\n",
        "\n",
        "    # If less than n words\n",
        "    if len(tweet_processed_lemmas) > min_words:\n",
        "      tweet_data = {\n",
        "          \"id\": tweet_id,\n",
        "          \"created_at\": tweet_created_at,\n",
        "          \"raw_text\": tweet_full_text,\n",
        "          \"lemma_text\": ' '.join(tweet_processed_lemmas),\n",
        "          \"hashtags\": tweet_hashtags,\n",
        "          \"mentions\": tweet_mentions,\n",
        "          \"user\": {\n",
        "              \"name\": tweet_user_name,\n",
        "              \"screen_name\": tweet_user_screen_name,\n",
        "              \"description\": tweet_user_description\n",
        "          }\n",
        "      }\n",
        "\n",
        "      # Append all the data\n",
        "      data.append(tweet_data)\n",
        "    \n",
        "  return data\n",
        "\n",
        "def clean_tweet_text(tweet_text):\n",
        "  tweet_text = tpre.clean(tweet_text)\n",
        "  tweet_text = re.sub(\"&gt;|&lt;|&amp;\", \"\", tweet_text)\n",
        "  tweet_text = remove_emojis(tweet_text)\n",
        "  return tweet_text\n",
        "\n",
        "def remove_emojis(data):\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', data)\n",
        "\n",
        "def load_tweets(file_name):\n",
        "  \n",
        "  # Load data\n",
        "  data = []\n",
        "\n",
        "  with open(file_name, 'r', encoding='utf8') as sample_data:\n",
        "    data = json.load(sample_data)\n",
        "\n",
        "  return data\n",
        "\n",
        "def save_tweets(data, dir, file_name):\n",
        "  with open(f'{dir}/{file_name}.json', 'w+', encoding='utf8') as outdata:\n",
        "    json.dump(data, outdata, ensure_ascii=False)\n",
        "\n",
        "def load_and_preprocess(cpipeline, data_dir, only_load, tweet_stop_words=[], tweet_upos=[], min_words=4, verbose=False, debug=False):\n",
        "  d = []\n",
        "  if only_load:\n",
        "    d = load_tweets(data_dir)\n",
        "  else:\n",
        "    d = preprocess_tweets(cpipeline, load_tweets(data_dir), tweet_stop_words=tweet_stop_words, tweet_upos=tweet_upos, min_words=min_words, verbose=verbose, debug=debug)\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ykd9Mmx5e-iy"
      },
      "outputs": [],
      "source": [
        "# Setting configuration\n",
        "\n",
        "# Path to unpreprocessed tweet data\n",
        "\n",
        "YEAR = 2021\n",
        "EPOCH = 2\n",
        "BATCH = 1\n",
        "\n",
        "tweet_data_path = f'unpreprocess/{YEAR}-{EPOCH}/{YEAR}_{EPOCH}_{BATCH}.json'\n",
        "\n",
        "# Path to save processed tweet data\n",
        "SAVING = True\n",
        "tweet_save_path = f'preprocess/{YEAR}-{EPOCH}'\n",
        "\n",
        "# Preprocessing configuration\n",
        "preprocess_config = {\n",
        "    'only_load': False,\n",
        "    'min_words': 4,\n",
        "    'verbose': True,\n",
        "    'debug': False,\n",
        "    'tweet_upos': ['PUNCT', 'NUM', 'SYM', 'CCONJ', 'INTJ'],\n",
        "    'tweet_stop_words': ['http', 'https', 'rt', 'oz']\n",
        "}\n",
        "\n",
        "# Classla configuration\n",
        "classla_conf = {\n",
        "  #'processors': 'tokenize, lemma',\n",
        "  'lang': 'sl',\n",
        "  'pos_lemma_pretag' : True,\n",
        "  'use_gpu': True\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YNxV64Te-iz",
        "outputId": "60e7ab8c-7eb0-4ca4-8c01-c58910988ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-09 12:07:13 INFO: Loading these models for language: sl (Slovenian):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | standard |\n",
            "| pos       | standard |\n",
            "| lemma     | standard |\n",
            "| depparse  | standard |\n",
            "| ner       | standard |\n",
            "========================\n",
            "\n",
            "2022-08-09 12:07:13 INFO: Use device: gpu\n",
            "2022-08-09 12:07:13 INFO: Loading: tokenize\n",
            "2022-08-09 12:07:13 INFO: Loading: pos\n",
            "2022-08-09 12:07:33 INFO: Loading: lemma\n",
            "2022-08-09 12:07:43 INFO: Loading: depparse\n",
            "2022-08-09 12:07:44 INFO: Loading: ner\n",
            "2022-08-09 12:07:44 INFO: Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "# Loading preprocessors\n",
        "\n",
        "# Classla preprocessor\n",
        "classla_pipeline = classla.Pipeline(**classla_conf)\n",
        "\n",
        "# Tweet preprocessor\n",
        "tpre.set_options(tpre.OPT.URL, tpre.OPT.MENTION, tpre.OPT.HASHTAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oI5rRNxe-i0",
        "outputId": "d5ade5a5-da1a-4619-c726-f380076e8d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Progress: 0/10000\n",
            "-- Progress: 1000/10000\n",
            "-- Time elapsed: 38.14611029624939s\n",
            "-- Tweets preprocessed: 274\n",
            "-- Progress: 2000/10000\n",
            "-- Time elapsed: 36.95746469497681s\n",
            "-- Tweets preprocessed: 568\n",
            "-- Progress: 3000/10000\n",
            "-- Time elapsed: 47.25840950012207s\n",
            "-- Tweets preprocessed: 931\n",
            "-- Progress: 4000/10000\n",
            "-- Time elapsed: 53.898706912994385s\n",
            "-- Tweets preprocessed: 1324\n",
            "-- Progress: 5000/10000\n",
            "-- Time elapsed: 49.66877889633179s\n",
            "-- Tweets preprocessed: 1727\n",
            "-- Progress: 6000/10000\n",
            "-- Time elapsed: 47.81388068199158s\n",
            "-- Tweets preprocessed: 2106\n",
            "-- Progress: 7000/10000\n",
            "-- Time elapsed: 44.154149770736694s\n",
            "-- Tweets preprocessed: 2445\n",
            "-- Progress: 8000/10000\n",
            "-- Time elapsed: 41.39900350570679s\n",
            "-- Tweets preprocessed: 2766\n",
            "-- Progress: 9000/10000\n",
            "-- Time elapsed: 49.98219013214111s\n",
            "-- Tweets preprocessed: 3157\n",
            "Saving preprocessed tweets to /content/drive/MyDrive/Diploma/Data/preprocess/2021-2...\n",
            "- Batch summary:\n",
            "-- Batch length: 3542\n"
          ]
        }
      ],
      "source": [
        "# MAIN CODE\n",
        "\n",
        "# Preprocess tweet data\n",
        "preprocessed_tweet_data = load_and_preprocess(classla_pipeline, f'{root_dir}/{tweet_data_path}', **preprocess_config)\n",
        "\n",
        "if SAVING:\n",
        "    print(f'Saving preprocessed tweets to {root_dir}/{tweet_save_path}...')\n",
        "    save_tweets(preprocessed_tweet_data, f'{root_dir}/{tweet_save_path}', f'{YEAR}_{EPOCH}_{BATCH}')\n",
        "\n",
        "# Print summary\n",
        "print(f'- Batch summary:')\n",
        "print(f'-- Batch length: {len(preprocessed_tweet_data)}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Tweet preprocessing.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "a37e6ef41f9041754ceb21f3dda61e636f9e402d0d6f0468955885061f3c932e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}