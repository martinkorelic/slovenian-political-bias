{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3mRyXJABvQm"
      },
      "source": [
        "# Predobdelava podatkov\n",
        "\n",
        "## Okolje\n",
        "\n",
        "Vzpostavitev okolja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h1jsXVwCyHd"
      },
      "outputs": [],
      "source": [
        "!pip install tweet-preprocessor\n",
        "!pip install classla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Tn4J4JIBulX"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import json\n",
        "import classla\n",
        "import re\n",
        "import time\n",
        "\n",
        "classla.download('sl')\n",
        "\n",
        "import preprocessor as tpre\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSu7MRUkEUDD"
      },
      "outputs": [],
      "source": [
        "# Setting constants\n",
        "\n",
        "LOCAL = False\n",
        "\n",
        "google_data_dir = \"/content/drive/MyDrive/Diploma/Data\"\n",
        "local_data_dir = \"/data\"\n",
        "\n",
        "root_dir = \"\"\n",
        "if LOCAL:\n",
        "    root_dir = local_data_dir\n",
        "else:\n",
        "    root_dir = google_data_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eHCHjqKD4jR"
      },
      "source": [
        "## Funkcije in razredi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUGMXt-RE2mY"
      },
      "outputs": [],
      "source": [
        "def preprocess_tweets(preprocess_pipeline, tweets, tweet_stop_words=[], tweet_upos=[], min_words=4, verbose=False, debug=False):\n",
        "\n",
        "  stop_words = stopwords.words('slovene')\n",
        "  stop_words.extend(tweet_stop_words)\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop_words = list(set(stop_words))\n",
        "\n",
        "  data = []\n",
        "  start_time = 0\n",
        "\n",
        "  for index, tweet in enumerate(tweets):\n",
        "\n",
        "    if index%1000 == 0 and verbose:\n",
        "      # Import time\n",
        "      print(f'-- Progress: {index}/{len(tweets)}')\n",
        "      if index >= 1000:\n",
        "        print(f'-- Time elapsed: {time.time() - start_time}s')\n",
        "        print(f'-- Tweets preprocessed: {len(data)}')\n",
        "      start_time = time.time()\n",
        "\n",
        "    # Take attributes\n",
        "    tweet_full_text = tweet['full_text']\n",
        "\n",
        "    # Skip if retweet\n",
        "    if tweet_full_text.startswith(\"RT\"):\n",
        "      continue\n",
        "\n",
        "    tweet_id = tweet['id']\n",
        "    tweet_hashtags = tweet['entities']['hashtags']\n",
        "    tweet_mentions = tweet['entities']['mentions']\n",
        "    #tweet_hashtags = tweet['hashtags']\n",
        "    #tweet_mentions = tweet['mentions']\n",
        "    tweet_created_at = tweet['created_at']\n",
        "\n",
        "    tweet_user_name = tweet['user']['name']\n",
        "    tweet_user_screen_name = tweet['user']['screen_name']\n",
        "    tweet_user_description = clean_tweet_text(tweet['user']['description']).lower()\n",
        "\n",
        "    # Remove hashtags, mentions, links, emojis and others\n",
        "    tweet_full_text = clean_tweet_text(tweet_full_text)\n",
        "\n",
        "    if debug:\n",
        "      print(f'Raw text:\\n{tweet_full_text}')\n",
        "\n",
        "    # Preprocess with preprocessing pipeline\n",
        "    tweet_lemma_text = preprocess_pipeline(tweet_full_text)\n",
        "\n",
        "    tweet_processed_lemmas = []\n",
        "    for sentence in tweet_lemma_text.sentences:\n",
        "\n",
        "      sentence_words = []\n",
        "\n",
        "      for i, word in enumerate(sentence.words):\n",
        " \n",
        "        # If the tweet is a retweet\n",
        "        if word.lemma.lower() == 'rt' and i == 0:\n",
        "          break\n",
        "\n",
        "        if debug:\n",
        "          print(f'Lemma: {word.lemma.lower()} -------------- Upos: {word.upos}')\n",
        "\n",
        "        # Not punctuation and not number + clean stopwords\n",
        "        if word.upos not in tweet_upos and word.lemma.lower() not in stop_words:\n",
        "          sentence_words.append(word.lemma.lower())\n",
        "        elif debug:\n",
        "          print(f'Discarded word: {word.lemma}')\n",
        "\n",
        "      tweet_processed_lemmas.extend(sentence_words)\n",
        "    \n",
        "    if debug:\n",
        "      print(f'Preprocessed lemmas:\\n{tweet_processed_lemmas}')\n",
        "\n",
        "    # If less than n words\n",
        "    if len(tweet_processed_lemmas) > min_words:\n",
        "\n",
        "      tweet_data = {\n",
        "          \"id\": tweet_id,\n",
        "          \"created_at\": tweet_created_at,\n",
        "          \"raw_text\": tweet_full_text,\n",
        "          \"lemma_text\": ' '.join(tweet_processed_lemmas),\n",
        "          \"hashtags\": tweet_hashtags,\n",
        "          \"mentions\": tweet_mentions,\n",
        "          \"user\": {\n",
        "              \"name\": tweet_user_name,\n",
        "              \"screen_name\": tweet_user_screen_name,\n",
        "              \"description\": tweet_user_description\n",
        "          }\n",
        "      }\n",
        "\n",
        "      # Append all the data\n",
        "      data.append(tweet_data)\n",
        "    \n",
        "  return data\n",
        "\n",
        "def clean_tweet_text(tweet_text):\n",
        "  tweet_text = tpre.clean(tweet_text)\n",
        "  tweet_text = re.sub(\"&gt;|&lt;|&amp;\", \"\", tweet_text)\n",
        "  tweet_text = remove_emojis(tweet_text)\n",
        "  return tweet_text\n",
        "\n",
        "def remove_emojis(data):\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', data)\n",
        "\n",
        "def load_tweets(file_name):\n",
        "  \n",
        "  # Load data\n",
        "  data = []\n",
        "\n",
        "  with open(file_name, 'r', encoding='utf8') as sample_data:\n",
        "    data = json.load(sample_data)\n",
        "\n",
        "  return data\n",
        "\n",
        "def save_tweets(data, dir, file_name):\n",
        "  with open(f'{dir}/{file_name}.json', 'w+', encoding='utf8') as outdata:\n",
        "    json.dump(data, outdata, ensure_ascii=False)\n",
        "\n",
        "def load_and_preprocess(cpipeline, data_dir, only_load, tweet_stop_words=[], tweet_upos=[], min_words=4, verbose=False, debug=False):\n",
        "  d = []\n",
        "  if only_load:\n",
        "    d = load_tweets(data_dir)\n",
        "  else:\n",
        "    d = preprocess_tweets(cpipeline, load_tweets(data_dir), tweet_stop_words=tweet_stop_words, tweet_upos=tweet_upos, min_words=min_words, verbose=verbose, debug=debug)\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ykd9Mmx5e-iy"
      },
      "outputs": [],
      "source": [
        "# Setting configuration\n",
        "\n",
        "# Path to unpreprocessed tweet data\n",
        "\n",
        "YEAR = 2021\n",
        "EPOCH = 41\n",
        "BATCHES = (7, 11)\n",
        "\n",
        "#tweet_data_path = f'unpreprocess/{YEAR}-{EPOCH}/{YEAR}_{EPOCH}_{BATCH}.json'\n",
        "\n",
        "# Path to save processed tweet data\n",
        "SAVING = True\n",
        "tweet_save_path = f'preprocess/{YEAR}-{EPOCH}'\n",
        "\n",
        "# Preprocessing configuration\n",
        "preprocess_config = {\n",
        "    'only_load': False,\n",
        "    'min_words': 4,\n",
        "    'verbose': True,\n",
        "    'debug': False,\n",
        "    'tweet_upos': ['PUNCT', 'NUM', 'SYM', 'CCONJ', 'INTJ'],\n",
        "    'tweet_stop_words': ['http', 'https', 'rt', 'oz']\n",
        "}\n",
        "\n",
        "# Classla configuration\n",
        "classla_conf = {\n",
        "  #'processors': 'tokenize, lemma',\n",
        "  'lang': 'sl',\n",
        "  'pos_lemma_pretag' : True,\n",
        "  'use_gpu': True\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YNxV64Te-iz"
      },
      "outputs": [],
      "source": [
        "# Loading preprocessors\n",
        "\n",
        "# Classla preprocessor\n",
        "classla_pipeline = classla.Pipeline(**classla_conf)\n",
        "\n",
        "# Tweet preprocessor\n",
        "tpre.set_options(tpre.OPT.URL, tpre.OPT.MENTION, tpre.OPT.HASHTAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oI5rRNxe-i0"
      },
      "outputs": [],
      "source": [
        "# MAIN CODE\n",
        "\n",
        "# Preprocess tweet data batches\n",
        "\n",
        "for batch in range(*BATCHES):\n",
        "\n",
        "  print(f'- Batch summary #{batch}:')\n",
        "\n",
        "  tweet_data_path = f'unpreprocess/{YEAR}-{EPOCH}/{YEAR}_{EPOCH}_{batch}.json'\n",
        "\n",
        "  preprocessed_tweet_data = load_and_preprocess(classla_pipeline, f'{root_dir}/{tweet_data_path}', **preprocess_config)\n",
        "\n",
        "  if SAVING:\n",
        "      print(f'Saving preprocessed tweets to {root_dir}/{tweet_save_path}...')\n",
        "      save_tweets(preprocessed_tweet_data, f'{root_dir}/{tweet_save_path}', f'{YEAR}_{EPOCH}_{batch}')\n",
        "\n",
        "  # Print summary\n",
        "  print(f'-- Batch length: {len(preprocessed_tweet_data)}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "a37e6ef41f9041754ceb21f3dda61e636f9e402d0d6f0468955885061f3c932e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}